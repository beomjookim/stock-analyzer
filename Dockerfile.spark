FROM bitnami/spark:3.3.1

USER root

# 기본 패키지 설치
RUN apt-get update && apt-get install -y \
    python3-pip \
    curl \
    apt-transport-https \
    ca-certificates \
    gnupg \
    openjdk-11-jdk

# GCP SDK 설치
RUN curl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - && \
    echo "deb https://packages.cloud.google.com/apt cloud-sdk main" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && \
    apt-get update && apt-get install -y google-cloud-sdk

# Spark JAR 디렉토리 생성
RUN mkdir -p /opt/spark/jars

# GCS Hadoop Connector 다운로드 (GCS 지원)
RUN curl -o /opt/spark/jars/gcs-connector-hadoop3-2.2.5.jar https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-2.2.5.jar

# Hadoop AWS JAR 추가 (필요할 경우)
RUN curl -o /opt/spark/jars/hadoop-aws-3.2.0.jar https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.2.0/hadoop-aws-3.2.0.jar
RUN curl -o /opt/spark/jars/aws-java-sdk-bundle-1.11.375.jar https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.375/aws-java-sdk-bundle-1.11.375.jar

# BigQuery Connector 추가 (BigQuery 지원)
RUN curl -o /opt/spark/jars/spark-bigquery-with-dependencies_2.12-0.29.0.jar \
    https://repo1.maven.org/maven2/com/google/cloud/spark/spark-bigquery-with-dependencies_2.12/0.29.0/spark-bigquery-with-dependencies_2.12-0.29.0.jar


# Google Cloud BigQuery 라이브러리 설치
RUN pip3 install google-cloud-bigquery

# Python 라이브러리 설치
RUN pip3 install --no-cache-dir \
    google-cloud-storage \
    google-cloud-bigquery \
    yfinance \
    pandas \
    requests \
    beautifulsoup4 \
    pyspark \
    pyarrow

# Spark 환경 설정 (GCS & BigQuery 연동)
ENV SPARK_CLASSPATH="/opt/spark/jars/gcs-connector-hadoop3-2.2.5.jar:/opt/spark/jars/hadoop-aws-3.2.0.jar:/opt/spark/jars/aws-java-sdk-bundle-1.11.375.jar:/opt/spark/jars/spark-bigquery-with-dependencies_2.12-0.29.0.jar"

RUN echo "spark.jars=/opt/spark/jars/gcs-connector-hadoop3-2.2.5.jar,/opt/spark/jars/spark-bigquery-with-dependencies_2.12-0.29.0.jar" >> /opt/bitnami/spark/conf/spark-defaults.conf
RUN echo "spark.hadoop.fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem" >> /opt/bitnami/spark/conf/spark-defaults.conf
RUN echo "spark.hadoop.fs.gs.auth.service.account.enable=true" >> /opt/bitnami/spark/conf/spark-defaults.conf
RUN echo "spark.hadoop.google.cloud.auth.service.account.json.keyfile=/opt/keys/gcs-key.json" >> /opt/bitnami/spark/conf/spark-defaults.conf

# BigQuery 관련 Spark 설정 추가
RUN echo "spark.jars.packages=com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.29.0" >> /opt/bitnami/spark/conf/spark-defaults.conf
RUN echo "spark.sql.catalogImplementation=hive" >> /opt/bitnami/spark/conf/spark-defaults.conf
RUN echo "spark.hadoop.fs.gs.auth.service.account.json.keyfile=/opt/keys/gcs-key.json" >> /opt/bitnami/spark/conf/spark-defaults.conf

# 환경 변수 설정 (GCP 인증 자동 활성화)
ENV GOOGLE_APPLICATION_CREDENTIALS="/opt/keys/gcs-key.json"

# 컨테이너 실행 시, GCP 인증 후 Spark 마스터 실행
ENTRYPOINT ["/bin/bash", "-c", "gcloud auth activate-service-account --key-file=/opt/keys/gcs-key.json && /opt/bitnami/spark/bin/spark-class org.apache.spark.deploy.master.Master"]

WORKDIR /opt/spark

COPY ./src /opt/spark/src