from concurrent.futures import ThreadPoolExecutor
from google.cloud import storage
import yfinance as yf
import pandas as pd
import os
import datetime
from fetch_tickers import get_top_50_sp500_tickers
from io import StringIO
import time

# GCS ì—…ë¡œë“œ í•¨ìˆ˜
def upload_to_gcs(bucket_name, destination_blob_name, dataframe):
    """Uploads a Pandas DataFrame to Google Cloud Storage directly from memory."""
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(destination_blob_name)

    # ğŸ”¹ ì»¬ëŸ¼ ìˆœì„œ ê°•ì œ ì •ë ¬
    expected_columns = [
        "Ticker", "Date", "Open", "High", "Low", "Close", "Volume",
        "Market_Cap", "PE_Ratio", "PB_Ratio", "Dividend_Yield", "EPS",
        "52_Week_High", "52_Week_Low"
    ]
    dataframe = dataframe[expected_columns]

    # ğŸ”¹ Date ì»¬ëŸ¼ì„ YYYY-MM-DD í¬ë§·ìœ¼ë¡œ ë³€í™˜
    dataframe["Date"] = pd.to_datetime(dataframe["Date"]).dt.strftime('%Y-%m-%d')


    # DataFrameì„ CSV í¬ë§·ìœ¼ë¡œ ë©”ëª¨ë¦¬ì— ì €ì¥í•œ í›„ ì—…ë¡œë“œ
    csv_buffer = StringIO()
    dataframe.to_csv(csv_buffer, index=False)
    blob.upload_from_string(csv_buffer.getvalue(), content_type="text/csv")

    print(f"âœ… Data successfully uploaded to gs://{bucket_name}/{destination_blob_name}")

def fetch_single_stock(ticker, period="1y"):
    """Fetch stock data for a single ticker."""
    try:
        stock = yf.Ticker(ticker)
        hist = stock.history(period=period)

        if hist.empty:
            return None

        # ì›ë³¸ ë°ì´í„° ì €ì¥ (ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°)
        hist = hist[["Open", "High", "Low", "Close", "Volume"]]
        hist["Ticker"] = ticker
        hist.reset_index(inplace=True)

        # ì›ë³¸ ë°ì´í„°ì— ìœ ì§€í•´ì•¼ í•˜ëŠ” ì¬ë¬´ ì •ë³´ ì¶”ê°€
        info = stock.info
        hist["Market_Cap"] = info.get("marketCap", None)
        hist["PE_Ratio"] = info.get("trailingPE", None)
        hist["PB_Ratio"] = info.get("priceToBook", None)
        hist["Dividend_Yield"] = info.get("trailingAnnualDividendYield", None)
        hist["EPS"] = info.get("trailingEps", None)
        hist["52_Week_High"] = info.get("fiftyTwoWeekHigh", None)
        hist["52_Week_Low"] = info.get("fiftyTwoWeekLow", None)

        return hist

    except Exception as e:
        print(f"âš  Error fetching data for {ticker}: {e}")
        return None

def fetch_stock_data(tickers, period="1y"):
    """Fetch stock data for multiple tickers using multi-threading."""
    stock_data = []

    # ğŸ”¹ ThreadPoolExecutorë¥¼ ì‚¬ìš©í•œ ë³‘ë ¬ API í˜¸ì¶œ (ìµœëŒ€ 10ê°œ ë™ì‹œ ì‹¤í–‰)
    with ThreadPoolExecutor(max_workers=10) as executor:
        results = list(executor.map(lambda t: fetch_single_stock(t, period), tickers))

    # None ê°’ ì œì™¸ í›„ ë³‘í•©
    stock_data = [r for r in results if r is not None]

    if stock_data:
        df = pd.concat(stock_data, ignore_index=True)

        # ğŸ”¹ ë‚ ì§œ ê¸°ì¤€ ì •ë ¬ (ìµœì‹  ë°ì´í„°ë¶€í„° ìˆœì„œëŒ€ë¡œ)
        df.sort_values(by=["Ticker", "Date"], ascending=[True, True], inplace=True)

        return df

    return pd.DataFrame()  # ë¹ˆ ë°ì´í„°í”„ë ˆì„ ë°˜í™˜

if __name__ == "__main__":

    # start_time = time.time()  # ì‹œì‘ ì‹œê°„ ê¸°ë¡

    tickers = get_top_50_sp500_tickers()
    stock_df = fetch_stock_data(tickers, period="1y")
    today = datetime.datetime.today().strftime('%Y%m%d')

    if not stock_df.empty:
        # GCS ì €ì¥ ê²½ë¡œ ì„¤ì •
        BUCKET_NAME = os.getenv("BUCKET_NAME")  # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°
        GCS_PATH = f"collected/sp500_top50_{today}.csv"  # GCS ë‚´ ì €ì¥ ê²½ë¡œ

        # ì›ë³¸ ë°ì´í„°ë§Œ GCSì— ì €ì¥
        upload_to_gcs(BUCKET_NAME, GCS_PATH, stock_df)
    else:
        print("âš  No data fetched. Check API availability.")

    # end_time = time.time()  # ì¢…ë£Œ ì‹œê°„ ê¸°ë¡
    # execution_time = end_time - start_time  # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
    # print(f"â± Execution Time: {execution_time:.4f} seconds")